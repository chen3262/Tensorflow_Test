{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression example\n",
    "- Compare three ways to optimize parameters\n",
    "    * Hand crafted gradient decent\n",
    "    * Tensorflow (GradientDescentOptimizer)\n",
    "    * Keras (Adagrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_data_path, train_label_path, test_data_path):\n",
    "    X_train = pd.read_csv(train_data_path, sep=',', header=0)\n",
    "    X_train = np.array(X_train.values)\n",
    "    Y_train = pd.read_csv(train_label_path, sep=',', header=0)\n",
    "    Y_train = np.array(Y_train.values)\n",
    "    X_test = pd.read_csv(test_data_path, sep=',', header=0)\n",
    "    X_test = np.array(X_test.values)\n",
    "\n",
    "    return (X_train, Y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, Y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "    return (X[randomize], Y[randomize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_valid_set(X_all, Y_all, percentage):\n",
    "    all_data_size = len(X_all)\n",
    "    valid_data_size = int(floor(all_data_size * percentage))\n",
    "\n",
    "    X_all, Y_all = _shuffle(X_all, Y_all)\n",
    "\n",
    "    X_train, Y_train = X_all[0:valid_data_size], Y_all[0:valid_data_size]\n",
    "    X_valid, Y_valid = X_all[valid_data_size:], Y_all[valid_data_size:]\n",
    "\n",
    "    return X_train, Y_train, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X_all, X_test):\n",
    "    # Feature normalization with train and test X\n",
    "    X_train_test = np.concatenate((X_all, X_test))\n",
    "    mu = (sum(X_train_test) / X_train_test.shape[0])\n",
    "    sigma = np.std(X_train_test, axis=0)\n",
    "    mu = np.tile(mu, (X_train_test.shape[0], 1))\n",
    "    sigma = np.tile(sigma, (X_train_test.shape[0], 1))\n",
    "    X_train_test_normed = (X_train_test - mu) / sigma\n",
    "\n",
    "    # Split to train, test again\n",
    "    X_all = X_train_test_normed[0:X_all.shape[0]]\n",
    "    X_test = X_train_test_normed[X_all.shape[0]:]\n",
    "    return X_all, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    res = 1 / (1.0 + np.exp(-z))\n",
    "    return np.clip(res, 1e-8, 1-(1e-8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File IO\n",
    "X_all, Y_all, X_test = load_data('./X_train.dms', './Y_train.dms', './X_test.dms');\n",
    "X_all, X_test = normalize(X_all, X_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a 10%-validation set from the training set\n",
    "valid_set_percentage = 0.1;\n",
    "X_train, Y_train, X_valid, Y_valid = split_valid_set(X_all, Y_all, valid_set_percentage);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_epochs = 200\n",
    "batch_size = 32\n",
    "step_num = int(floor(len(X_train) / batch_size))\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Hand crafted gradient decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w_t = np.zeros((106,))\n",
    "b_t = np.zeros((1,))\n",
    "\n",
    "# Training Cycle\n",
    "for epoch in range(hm_epochs):\n",
    "    \n",
    "    # Random shuffle\n",
    "    X_train, Y_train = _shuffle(X_train, Y_train)\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    # Loop over all batches\n",
    "    for i in range(step_num):\n",
    "        X = X_train[i*batch_size:(i+1)*batch_size]\n",
    "        Y = Y_train[i*batch_size:(i+1)*batch_size]\n",
    "        z_t = np.dot(X, np.transpose(w_t)) + b_t\n",
    "        y_t = sigmoid(z_t)\n",
    "\n",
    "        cross_entropy = -1 * (np.dot(np.squeeze(Y), np.log(y_t)) + np.dot((1 - np.squeeze(Y)), np.log(1 - y_t)))\n",
    "        total_loss += cross_entropy/batch_size/step_num\n",
    "\n",
    "        w_grad = np.sum(-1 * X * (np.squeeze(Y) - y_t).reshape((batch_size,1)), axis=0)\n",
    "        b_grad = np.sum(-1 * (np.squeeze(Y) - y_t))\n",
    "        w_t -= 0.01 * w_grad\n",
    "        b_t -= 0.01 * b_grad\n",
    "    \n",
    "    # Display logs per epoch step\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        # Accuracy\n",
    "        z = (np.dot(X_valid, np.transpose(w_t)) + b_t)\n",
    "        y = sigmoid(z)\n",
    "        y_ = np.around(y)\n",
    "        result = (np.squeeze(Y_valid) == y_)\n",
    "        \n",
    "        print('Epoch', '%04d' %(epoch+1), 'loss: ', \"{:.9f}\".format(total_loss), 'acc: ', \"{:.9f}\".format(float(result.sum()) / len(X_valid)),'(Hand crafted)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([106,1]))\n",
    "b = tf.Variable([0.0])\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder('float', [None, 106])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "\n",
    "# Prediction and cost function\n",
    "prediction = tf.matmul(x, W) + b\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "# Run the initializer\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Accuracy\n",
    "predict_op  = tf.greater_equal(prediction, tf.zeros_like(prediction))\n",
    "correct_op  = tf.equal(tf.cast(predict_op, tf.float32), y)\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_op, tf.float32))\n",
    "\n",
    "# Training Cycle\n",
    "for epoch in range(hm_epochs):\n",
    "    \n",
    "    # Random shuffle\n",
    "    X_train, Y_train = _shuffle(X_train, Y_train)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Loop over all batches\n",
    "    for i in range(step_num):\n",
    "        epoch_x = X_train[i*batch_size:(i+1)*batch_size]\n",
    "        epoch_y = Y_train[i*batch_size:(i+1)*batch_size]\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "        epoch_loss += c/step_num\n",
    "    \n",
    "    # Display logs per epoch step\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        accuracy = sess.run(accuracy_op, {x:X_valid, y:Y_valid})\n",
    "        print('Epoch', '%04d' %(epoch+1),'loss: ', \"{:.9f}\".format(epoch_loss),'acc: ', \"{:.9f}\".format(accuracy))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "output_dim = nb_classes = 1;\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(output_dim, input_dim=X_train.shape[1], activation='sigmoid')) \n",
    "model.compile(optimizer='adagrad', loss='binary_crossentropy', metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "                    epochs=hm_epochs,verbose=1, validation_data=(X_valid, Y_valid)) \n",
    "score = model.evaluate(X_valid, Y_valid, verbose=0) \n",
    "print('Valid score:', score[0]) \n",
    "print('Valid accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 172 µs, sys: 180 µs, total: 352 µs\n",
      "Wall time: 251 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "A = np.random.random((3, 3))\n",
    "B = np.random.random((3, 3))\n",
    "np.dot(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.3 ms, sys: 1.97 ms, total: 77.3 ms\n",
      "Wall time: 77.2 ms\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    A_ = tf.placeholder('float',[3,3])\n",
    "    B_ = tf.placeholder('float',[3,3])\n",
    "    %%time sess.run(tf.tensordot(A_,B_,axes=1), feed_dict={A_: A, B_: B})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
